{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c9cd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1e42e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 43351.98it/s]\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = GLiNER.from_pretrained(\"urchade/gliner_multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280e99dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 parquet files\n"
     ]
    }
   ],
   "source": [
    "# get news data\n",
    "news_folder = '/workspaces/ner_news_malay/scraper/news_id'\n",
    "parquet_files = glob.glob(os.path.join(news_folder, '*.parquet'))\n",
    "print(f\"found {len(parquet_files)} parquet files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645e64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from files\n",
    "corpus_text = []\n",
    "for file_path in parquet_files:\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        corpus_text.extend(df['Title'].dropna().str.lower().tolist())\n",
    "        corpus_text.extend(df['Summary'].dropna().str.lower().tolist())\n",
    "    except Exception as e:\n",
    "        print(f\"error processing {file_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd0e581f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 8 sentences\n"
     ]
    }
   ],
   "source": [
    "# save corpus to text file\n",
    "corpus_file = '/workspaces/ner_news_malay/model_gliner/malay_news_corpus.txt'\n",
    "\n",
    "with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "    for text in corpus_text:\n",
    "        f.write(text + '\\n')\n",
    "print(f\"corpus size: {len(corpus_text)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a08b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Read text from file\n",
    "with open(\"malay_news_corpus.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Define target labels\n",
    "labels = [\"GPE\",\"PERSON\",\"ORG\",\"FAC\",\"MONEY\",\"NORP\",\"LOC\",\"PRODUCT\",\"EVENT\",\n",
    "          \"PERCENT\",\"WORK_OF_ART\",\"TIME\",\"ORDINAL\",\"CARDINAL\",\"QUANTITY\",\"LAW\"]\n",
    "\n",
    "# Chunk processing parameters\n",
    "CHUNK_SIZE = 300  # Max tokens per chunk\n",
    "OVERLAP = 50      # Token overlap between chunks\n",
    "WORD_PATTERN = r'\\b\\w+\\b'  # Word boundary pattern\n",
    "\n",
    "# Split text into words\n",
    "words = re.findall(WORD_PATTERN, text)\n",
    "all_entities = []\n",
    "\n",
    "# Process text in chunks with overlap\n",
    "start = 0\n",
    "while start < len(words):\n",
    "    # Calculate chunk end with overlap\n",
    "    end = start + CHUNK_SIZE\n",
    "    if end > len(words):\n",
    "        end = len(words)\n",
    "    \n",
    "    # Reconstruct text chunk\n",
    "    chunk_text = ' '.join(words[start:end])\n",
    "    \n",
    "    # Find chunk boundaries in original text\n",
    "    chunk_start_idx = text.find(chunk_text)\n",
    "    if chunk_start_idx == -1:  # Handle edge case\n",
    "        chunk_start_idx = 0\n",
    "    chunk_end_idx = chunk_start_idx + len(chunk_text)\n",
    "    \n",
    "    # Process chunk with model\n",
    "    entities = model.predict_entities(chunk_text, labels)\n",
    "    \n",
    "    # Adjust entity positions to original text\n",
    "    for entity in entities:\n",
    "        entity[\"start\"] += chunk_start_idx\n",
    "        entity[\"end\"] += chunk_start_idx\n",
    "        all_entities.append(entity)\n",
    "    \n",
    "    # Move to next chunk with overlap\n",
    "    start = end - OVERLAP\n",
    "    if start < 0:\n",
    "        start = 0\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "seen_entities = set()\n",
    "unique_entities = []\n",
    "for entity in all_entities:\n",
    "    # Create unique identifier using position and label\n",
    "    identifier = (entity[\"start\"], entity[\"end\"], entity[\"label\"])\n",
    "    if identifier not in seen_entities:\n",
    "        seen_entities.add(identifier)\n",
    "        unique_entities.append(entity)\n",
    "\n",
    "# Sort entities by position in original text\n",
    "unique_entities.sort(key=lambda x: x[\"start\"])\n",
    "\n",
    "# Print results\n",
    "for entity in unique_entities:\n",
    "    entity_text = text[entity[\"start\"]:entity[\"end\"]]\n",
    "    print(f\"{entity_text} => {entity['label']}\")\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nTotal entities detected: {len(unique_entities)}\")\n",
    "label_counts = {}\n",
    "for entity in unique_entities:\n",
    "    label = entity[\"label\"]\n",
    "    label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "print(\"\\nEntity counts by type:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
