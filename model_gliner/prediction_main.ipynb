{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1798bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e9901b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 46474.28it/s]\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = GLiNER.from_pretrained(\"urchade/gliner_multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bc94add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get news data (parquet file)\n",
    "news_folder = '/workspaces/ner_news_malay/model_gliner'\n",
    "parquet_files = glob.glob(os.path.join(news_folder, '*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "108af78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from files\n",
    "corpus_text = []\n",
    "for file_path in parquet_files:\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        corpus_text.extend(df['Title'].dropna().str.lower().tolist())\n",
    "        corpus_text.extend(df['Summary'].dropna().str.lower().tolist())\n",
    "    except Exception as e:\n",
    "        print(f\"error processing {file_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "433b8dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 2040 sentences\n"
     ]
    }
   ],
   "source": [
    "# save corpus to text file\n",
    "corpus_file = '/workspaces/ner_news_malay/model_gliner/malay_news_corpus.txt'\n",
    "\n",
    "with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "    for text in corpus_text:\n",
    "        f.write(text + '\\n')\n",
    "print(f\"Corpus size: {len(corpus_text)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8505f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels\n",
    "labels = [\"GPE\",\"PERSON\",\"ORG\",\"FAC\",\"MONEY\",\"NORP\",\"LOC\",\"PRODUCT\",\"EVENT\",\n",
    "          \"PERCENT\",\"WORK_OF_ART\",\"TIME\",\"ORDINAL\",\"CARDINAL\",\"QUANTITY\",\"LAW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a301dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 449 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 566 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 403 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 498 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 435 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 727 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 464 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 920 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gliner/data_processing/processor.py:296: UserWarning: Sentence of length 443 has been truncated to 384\n",
      "  warnings.warn(f\"Sentence of length {len(tokens)} has been truncated to {max_len}\")\n"
     ]
    }
   ],
   "source": [
    "# process individual sentences\n",
    "all_entities = []\n",
    "current_position = 0\n",
    "\n",
    "for sentence in corpus_text:\n",
    "    if not sentence.strip():\n",
    "        current_position += len(sentence) + 1\n",
    "        continue\n",
    "        \n",
    "    # get predictions\n",
    "    entities = model.predict_entities(sentence, labels)\n",
    "    \n",
    "    # adjust position\n",
    "    for entity in entities:\n",
    "        entity[\"start\"] += current_position\n",
    "        entity[\"end\"] += current_position\n",
    "        all_entities.append(entity)\n",
    "    \n",
    "    # update position\n",
    "    current_position += len(sentence) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfdc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates\n",
    "seen = set()\n",
    "unique_entities = []\n",
    "for entity in all_entities:\n",
    "    identifier = (entity[\"text\"], entity[\"label\"], entity[\"start\"], entity[\"end\"])\n",
    "    if identifier not in seen:\n",
    "        seen.add(identifier)\n",
    "        unique_entities.append(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a113fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>jkm</td>\n",
       "      <td>ORG</td>\n",
       "      <td>0.951072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "      <td>utusan malaysia</td>\n",
       "      <td>ORG</td>\n",
       "      <td>0.684363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76</td>\n",
       "      <td>82</td>\n",
       "      <td>soniia</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>0.960096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>105</td>\n",
       "      <td>bukit kiara</td>\n",
       "      <td>LOC</td>\n",
       "      <td>0.654027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>108</td>\n",
       "      <td>123</td>\n",
       "      <td>utusan malaysia</td>\n",
       "      <td>LOC</td>\n",
       "      <td>0.667498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start  end             text   label     score\n",
       "0      6    9              jkm     ORG  0.951072\n",
       "1     60   75  utusan malaysia     ORG  0.684363\n",
       "2     76   82           soniia  PERSON  0.960096\n",
       "3     94  105      bukit kiara     LOC  0.654027\n",
       "4    108  123  utusan malaysia     LOC  0.667498"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to df\n",
    "df = pd.DataFrame(unique_entities)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65949520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10930 entries, 0 to 10929\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   start   10930 non-null  int64  \n",
      " 1   end     10930 non-null  int64  \n",
      " 2   text    10930 non-null  object \n",
      " 3   label   10930 non-null  object \n",
      " 4   score   10930 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 427.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca93976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "df.to_csv(\"results_main.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
