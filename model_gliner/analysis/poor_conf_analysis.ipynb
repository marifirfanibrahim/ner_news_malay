{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4605d53f",
   "metadata": {},
   "source": [
    "# Poor Confidence Analysis\n",
    "This analysis aims to improve the NER model by studying the predictions done by the model. This is done firstly by identifying the weakness or limitations discovered from the prediction results. So, by looking at the outputs, the confidence score can be the area of focus for this analysis.\n",
    "\n",
    "The idea of this analysis is to find out if the low confidence scores are justifiable. For example, is the confidence score related to the volume of the dataset, frequency of the labels, or word rarity? This analysis covers the:\n",
    "1. label frequency\n",
    "2. text frequency\n",
    "3. confidence score distribution\n",
    "4. text length\n",
    "5. text complexity\n",
    "\n",
    "This can be determined by answering these questions:\n",
    "1. Is there a correlation between label frequency and confidence scores?    (`label`)\n",
    "2. Are low-confidence predictions associated with text frequency/rarity?    (`text`)\n",
    "3. Do certain label types show systematic confidence patterns?              (`label`)\n",
    "4. Is confidence score correlated with text length?                         (`text`)\n",
    "5. Is confidence score correlated with complexity?                          (`text`)\n",
    "\n",
    "Based on these questions, \n",
    "\n",
    "* If low scores correlate with rare labels: Augment training data for underrepresented classes.\n",
    "* If low scores correlate with rare words: Add domain-specific vocabulary for training.\n",
    "* If low scores are consistent with certain label group: Focused annotation to cator specific labelings.\n",
    "\n",
    "If all are unclear: Review model architecture or hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ab93f",
   "metadata": {},
   "source": [
    "-------------\n",
    "------\n",
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68ae5b",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0450434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats             # correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a2b2f",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b4ba800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Arif Irfan Ibrahim\\AppData\\Local\\Temp\\ipykernel_12440\\252216309.py:2: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  df = pd.read_csv('ner_news_malay\\model_gliner\\data\\\\results_main.csv')\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ner_news_malay\\\\model_gliner\\\\data\\\\results_main.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# load csv\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mner_news_malay\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mmodel_gliner\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mresults_main.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'ner_news_malay\\\\model_gliner\\\\data\\\\results_main.csv'"
     ]
    }
   ],
   "source": [
    "# load csv\n",
    "df = pd.read_csv('ner_news_malay\\model_gliner\\data\\\\results_main.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e693f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4883ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize text\n",
    "df['text'] = df['text'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c340c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates\n",
    "df.drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a3bab",
   "metadata": {},
   "source": [
    "-------------\n",
    "--------\n",
    "## Exploratory Data Analysis\n",
    "This part aims to explore the dataset in hopes to uncover hidden perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c443f2c9",
   "metadata": {},
   "source": [
    "#### Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label distribution\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "label_freq = df['label'].value_counts().index\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "sns.countplot(x=df['label'],data=df, order=label_freq)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bfd543",
   "metadata": {},
   "source": [
    "From this graph, it is safe to assume that the high-frequency labels are the 3 highest, while the others are teh low-frequency labels. This is because the gap between the third and the fourth highest is relatively big (1500 vs 500), when compared to other inter-label gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa0479",
   "metadata": {},
   "source": [
    "#### High and Low Frequency Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e894a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "freq = df.drop(['start','end','text'], axis=1)\n",
    "freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a228731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list labels\n",
    "freq['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb593c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into higher and lower frequency groups\n",
    "high_freq_label = ['PERSON','ORG','LOC']\n",
    "low_freq_label = ['PRODUCT', 'QUANTITY', 'EVENT', 'GPE',\n",
    "       'CARDINAL', 'TIME', 'LAW', 'MONEY', 'PERCENT', 'WORK_OF_ART',\n",
    "       'NORP', 'FAC', 'ORDINAL']\n",
    "\n",
    "freq_high = freq[freq['label'].isin(high_freq_label)]\n",
    "freq_low = freq[freq['label'].isin(low_freq_label)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b89dc6",
   "metadata": {},
   "source": [
    "#### Confidence Score Disctribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean confidence score\n",
    "conf = df.groupby('label')['score'].mean().reset_index()\n",
    "conf = conf.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "bars = ax.bar(conf['label'], conf['score'], color='purple')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2,\n",
    "            height + 0.01, f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_title('Confidence Score Distribution')\n",
    "ax.set_xlabel('Entity Label')\n",
    "ax.set_ylabel('Average Confidence Score')\n",
    "ax.set_ylim(0.5, 0.9)\n",
    "ax.axhline(y=0.7,linestyle='--')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a75592",
   "metadata": {},
   "source": [
    "Based on this distribution, there are 5 labels that are considered to be low-confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d0240",
   "metadata": {},
   "source": [
    "#### Confidence Scores in High and Low Frequency Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6716778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean score\n",
    "label_score_mean_high = freq_high.groupby('label')['score'].mean().reset_index()\n",
    "label_score_mean_low = freq_low.groupby('label')['score'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010be65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort mean score\n",
    "label_score_mean_high = label_score_mean_high.sort_values('score', ascending=False)\n",
    "label_score_mean_low = label_score_mean_low.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# high frequency labels\n",
    "bars1 = ax1.bar(label_score_mean_high['label'], \n",
    "               label_score_mean_high['score'],\n",
    "               color='green')\n",
    "\n",
    "# labeling\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "            f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# setting\n",
    "ax1.set_title('High Frequency Entities')\n",
    "ax1.set_xlabel('Entity Label')\n",
    "ax1.set_ylabel('Average Confidence Score')\n",
    "ax1.set_ylim(0.5, 0.9)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# low frequency labels\n",
    "bars2 = ax2.bar(label_score_mean_low['label'], \n",
    "               label_score_mean_low['score'],\n",
    "               color='teal')\n",
    "\n",
    "# labeling\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "            f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# setting\n",
    "ax2.set_title('Low Frequency Entities')\n",
    "ax2.set_xlabel('Entity Label')\n",
    "ax2.set_ylabel('Average Confidence Score')\n",
    "ax2.set_ylim(0.5, 0.9)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# display\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ef062",
   "metadata": {},
   "source": [
    "Based on these graphs, the lowest confidence score for the high-frequency labels is around 76%, while the highest confidence score for the low-frequency labels is around 82%. This means that even if the label is relatively rare, the confidence score can still be high. Even so, it cannot be a certain that there is no correlation between the label rarity and the confidence score. Therefore, a correlation analysis can be done to further confirm this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02d2f4b",
   "metadata": {},
   "source": [
    "#### Low Rarity Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f678cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text frequency\n",
    "text_freq = df['text'].value_counts().reset_index() \n",
    "text_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts that appeared only once\n",
    "text_low = text_freq[text_freq['count'] == 1]\n",
    "text_low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d36ad6",
   "metadata": {},
   "source": [
    "From this, there are about 4000 texts that appeared only once. That's more than half of the whole dataset (6000 texts)! This is also while excluding the texts that appeared only twice or thrice.\n",
    "\n",
    "However, this is to be expected since texts with unique nomenclature or numbers might be a factor to this phenomenon. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543346c",
   "metadata": {},
   "source": [
    "#### Distribution of Confidence Score in Low Rarity Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ddb7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score for low rarity texts\n",
    "text_conf = df.groupby('text')['score'].mean().reset_index()  # get mean confidence score\n",
    "\n",
    "text_low_conf = pd.merge(text_freq, text_conf, on='text')\n",
    "text_low_conf = text_low_conf[text_low_conf['count'] == 1]\n",
    "text_low_conf.sort_values('score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62215b1f",
   "metadata": {},
   "source": [
    "Here, we can see that the rare texts has the confidence score at both high and low ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_low = df[df['score'] < 0.7]\n",
    "score_low.sort_values('score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf698c",
   "metadata": {},
   "source": [
    "There are about 5000 texts that has low confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16934948",
   "metadata": {},
   "source": [
    "#### Distribution of Text Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_len = df.copy()\n",
    "txt_len['chara'] = df['text'].str.len()\n",
    "txt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98749a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of text length\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# histogram\n",
    "sns.histplot(txt_len['chara'], bins=50, kde=True, color='teal')\n",
    "\n",
    "# mean line\n",
    "mean_length = txt_len['chara'].mean()\n",
    "plt.axvline(mean_length, color='red', linestyle='--', \n",
    "            label=f'Mean: {mean_length:.0f} chars')\n",
    "\n",
    "# median line\n",
    "median_length = txt_len['chara'].median()\n",
    "plt.axvline(median_length, color='blue', linestyle='--', \n",
    "            label=f'Median: {median_length:.0f} chars')\n",
    "\n",
    "# plot\n",
    "plt.title('Distribution of Text Lengths')\n",
    "plt.xlabel('Number of Characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eeb739",
   "metadata": {},
   "source": [
    "This distribution histogram is right-skewed. It comprises of mostly short texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a6a97",
   "metadata": {},
   "source": [
    "--------------\n",
    "--------------\n",
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910a95d",
   "metadata": {},
   "source": [
    "-------------\n",
    "### Q1: Is there a correlation between label frequency and confidence scores?\n",
    "Study on the mean confidence scores and label frequency correlation. This is to see if high frequency means high confidence score, or vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ba0e6",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f47625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataset\n",
    "q1_label = df['label'].value_counts().reset_index()         # get label frequency\n",
    "q1_conf = df.groupby('label')['score'].mean().reset_index() # get mean confidence score\n",
    "\n",
    "q1_label_analysis = pd.merge(q1_label, q1_conf, on='label')\n",
    "q1_label_analysis.sort_values('score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eccfa9",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "In this analysis, the tests that will be conducted are:\n",
    "1. Pearson correlation - to determine the linear relationship\n",
    "2. Spearman correlation - to determine the monotonic relationship\n",
    "3. Shapiro-Wilk test - to determine if its normally distributed\n",
    "\n",
    "**Null hypothesis:**\\\n",
    "There is no correlation between label frequency and confidence scores.\n",
    "\n",
    "**Alternative hypothesis:**\\\n",
    "There is a correlation between label frequency and confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b779f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson\n",
    "q1_pearson_corr, q1_p_value = stats.pearsonr(\n",
    "    q1_label_analysis['count'],\n",
    "    q1_label_analysis['score']\n",
    ")\n",
    "\n",
    "# spearman\n",
    "q1_spearman_rho, q1_p_spearman = stats.spearmanr(\n",
    "    q1_label_analysis['count'],\n",
    "    q1_label_analysis['score']\n",
    ")\n",
    "\n",
    "# normality test\n",
    "q1_shapiro_test = stats.shapiro(q1_label_analysis['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Spearman rho    : {q1_spearman_rho:.3f}\")\n",
    "print(f\"Spearman p      : {q1_p_spearman:.4f}\\n\")\n",
    "print(f\"Pearson r       : {q1_pearson_corr:.3f}\")\n",
    "print(f\"Pearson p       : {q1_p_value:.4f}\\n\")\n",
    "print(f\"Shapiro-Wilk W  : {q1_shapiro_test[0]:.3f}\")\n",
    "print(f\"Shapiro-Wilk p  : {q1_shapiro_test[1]:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e91d5",
   "metadata": {},
   "source": [
    "Based on these results:\n",
    "* There is a significant (p < 0.05) strong positive monotonic relationship (rho > 0.6). This means that as label frequency increases, confidence scores consistently increase, but not necessarily in a straight-line pattern - Spearman correlation test\n",
    "* There is a significant (p < 0.05) moderate positive linear relationship (r > 0.4, r < 0.6). This means that higher frequency moderately predicts higher confidence in a linear pattern. - Pearson correlation test\n",
    "* Since the rho > r, the monotonic relationship is stronger than the linear relationship. This means that even though higher frequency boosts the confidence score, the boost will decrease with higher frequency.\n",
    "* Since both of these tests are significant, the results are not random.\n",
    "* The data is normally distributed (p > 0.05). - Shapiro-Wilk test\n",
    "\n",
    "Therefore, we reject the null hypothesis.\n",
    "\n",
    "For fine-tuning, training data that focuses on the low-frequency labels can be used for augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter low-frequency labels in main dataset\n",
    "ft_q1 = df[df['label'].isin(low_freq_label)]\n",
    "ft_q1 = ft_q1[ft_q1['score'] < 0.7]\n",
    "ft_q1.sort_values('score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c4351",
   "metadata": {},
   "source": [
    "------------\n",
    "### Q2: Are low-confidence predictions correlated with text frequency/rarity?\n",
    "Study on the low confidence scores and text frequency. This is to see if lower rarity means lower confidence score, or vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c7cc5",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73967e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataset\n",
    "q2_text = df['text'].value_counts().reset_index()           # get text frequency\n",
    "q2_conf = df.groupby('text')['score'].mean().reset_index()  # get mean confidence score\n",
    "\n",
    "q2_text_analysis = pd.merge(q2_text, q2_conf, on='text')\n",
    "q2_text_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff0c5c",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "In this analysis, the tests that will be conducted are:\n",
    "1. Pearson correlation - to determine the linear relationship\n",
    "2. Spearman correlation - to determine the monotonic relationship\n",
    "\n",
    "**Null hypothesis:**\\\n",
    "There is no correlation between text frequency and confidence scores.\n",
    "\n",
    "**Alternative hypothesis:**\\\n",
    "There is a correlation between text frequency and confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9706be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson\n",
    "q2_pearson_corr, q2_p_value = stats.pearsonr(\n",
    "    q2_text_analysis['count'],\n",
    "    q2_text_analysis['score']\n",
    ")\n",
    "\n",
    "# spearman\n",
    "q2_spearman_rho, q2_p_spearman = stats.spearmanr(\n",
    "    q2_text_analysis['count'],\n",
    "    q2_text_analysis['score']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Spearman rho: {q2_spearman_rho:.3f}\")\n",
    "print(f\"Spearman p: {q2_p_spearman:.4f}\\n\")\n",
    "print(f\"Pearson r: {q2_pearson_corr:.3f}\")\n",
    "print(f\"Pearson p: {q2_p_value:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc49ea",
   "metadata": {},
   "source": [
    "Based on these results:\n",
    "* There is a significant (p < 0.05) but negligible monotonic relationship (rho < 0.1). This means that there is no meaningful relationship between text and confidence score. - Spearman correlation test\n",
    "* There is no significant (p > 0.05) linear relationship (r < 0.1). This means that there is no evidence of linear relationship between text and confidence score. - Pearson correlation test\n",
    "\n",
    "Therefore, we do not reject the null hypothesis.\n",
    "\n",
    "The fine-tuning should be aimed at other factors instead.\n",
    "\n",
    "This further justifies the EDA,  where the confidence scores are covered from both high and low ends from the texts, despite the rarity of the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f617cb2b",
   "metadata": {},
   "source": [
    "------------\n",
    "### Q3: Do certain label types show systematic confidence patterns?\n",
    "Analyze low confidence score distributions per label type. This is to see whether certain labels are under-trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72900bd8",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cb7f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deca5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and filter score < 0.7\n",
    "q3 = df.sort_values('score', ascending=False)\n",
    "q3 = q3[q3['score'] < 0.7]\n",
    "q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of labels\n",
    "print(f\"number of labels with low-scores    : {len(q3['label'].unique())}\")\n",
    "print(f\"number of labels on main            : {len(df['label'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7f8f3",
   "metadata": {},
   "source": [
    "It seems that there are low-confidence scores for all  types of lables. However, based on our EDA, the lables that has low-average confidence scores are the FAC, NORP, QUANTITY, CARDINAL, and ORDINAL (5 labels). This might be because of outliers in each label type. But to be sure, the weight of the low confidence scores and high confidence scores can be measured for each label. Since the number of the scores cannot be compared because of relative frequencies, percentages can be used for the comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abecf577",
   "metadata": {},
   "source": [
    "#### Compare Confidence Score of Individual Labels\n",
    "Get horizontal bar chart for each label (16 bar charts), with confidence score of 0.7 as the sperator for each chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04eeff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get back full dataset\n",
    "q3 = df\n",
    "q3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef395f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number labels of high and low confidence\n",
    "q3_conf = q3.groupby('label')['score'].agg(\n",
    "    high_conf=lambda x: (x >= 0.7).sum(),\n",
    "    low_conf=lambda x: (x < 0.7).sum(),\n",
    "    total='count'\n",
    ").reset_index()\n",
    "q3_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25614648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get percentages\n",
    "q3_conf['high_conf_pct'] = (q3_conf['high_conf'] / q3_conf['total'] * 100).round(1)\n",
    "q3_conf['low_conf_pct'] = (q3_conf['low_conf'] / q3_conf['total'] * 100).round(1)\n",
    "q3_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by high confidence percentage (descending)\n",
    "q3_conf = q3_conf.sort_values('high_conf_pct', ascending=False)\n",
    "q3_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeafc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "plt.figure(figsize=(14, 10))\n",
    "y_pos = np.arange(len(q3_conf))\n",
    "bar_height = 0.7\n",
    "\n",
    "# stacked horizontal bar charts\n",
    "low_bars = plt.barh(y_pos, q3_conf['low_conf_pct'], \n",
    "                    height=bar_height, color='brown', label='Confidence < 0.7')\n",
    "high_bars = plt.barh(y_pos, q3_conf['high_conf_pct'], \n",
    "                     left=q3_conf['low_conf_pct'], \n",
    "                     height=bar_height, color='teal', label='Confidence >= 0.7')\n",
    "\n",
    "# annotations\n",
    "for i, (low_pct, high_pct, total) in enumerate(zip(q3_conf['low_conf_pct'], \n",
    "                                                   q3_conf['high_conf_pct'], \n",
    "                                                   q3_conf['total'])):\n",
    "    # low confidence scores\n",
    "    if low_pct > 1:     # to ensure space\n",
    "        plt.text(low_pct/2, i, \n",
    "                 f'{low_pct:.1f}%\\n({int(q3_conf[\"low_conf\"].iloc[i])})', \n",
    "                 ha='center', va='center', color='white', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # high confidence scores\n",
    "    if high_pct > 1:    # to ensure space \n",
    "        plt.text(q3_conf['low_conf_pct'].iloc[i] + high_pct/2, i, \n",
    "                 f'{high_pct:.1f}%\\n({int(q3_conf[\"high_conf\"].iloc[i])})', \n",
    "                 ha='center', va='center', color='white', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # total\n",
    "    plt.text(105, i, f'n={total}')\n",
    "\n",
    "    # line at 50%\n",
    "\n",
    "# plot\n",
    "plt.axvline(x=50, color='grey', linestyle='--')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.12), ncol=2)\n",
    "plt.yticks(y_pos, q3_conf['label'])\n",
    "plt.title('Confidence Distribution by Label Type')\n",
    "plt.xlim(0, 110)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c2423d",
   "metadata": {},
   "source": [
    "There are a few key points that can be taken away from this chart, but to break it down further:\n",
    "\n",
    "Based on >= 50% high confidence score coverage(10 labels):\n",
    "* Even though 'MONEY','PERCENT', 'GPE', 'LAW', 'TIME', 'PRODUCT', and 'EVENT' are considered to be low-frequency labels, based on the EDA, they have high confidence score coverage.\n",
    "* The high-frequency labels, 'ORG', 'PERSON', and 'LOC' all have high confidence score coverage.\n",
    "* The 'PERCENT' may be an outlier, because despite having the lowest number of labels in this category, it is has the second highest high confidence score coverage. This might be because of the simplicity/less variability of the annotation i.e. the symbol '%' is used as the indication for the label.\n",
    "\n",
    "Based on < 50% high confidence score coverage(6 labels):\n",
    "* All of the labels in these category are the low-frequency labels based on the EDA.\n",
    "* The ' QUANTITY' label may in an outlier, because despite having the highest number of labels, about double from the second highest, it is has a poor high confidence score coverage. This might be because of the variability of the annotation, from being numbers.\n",
    "\n",
    "From this, a label-focused annotation can be done for the fine-tuning:\n",
    "* 'ORDINAL', 'CARDINAL', 'QUANTITY', 'NORP', 'WORK_OF_ART', and 'EVENT' should be the labels of focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a96676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get low high confidence covergage labels with low confidence\n",
    "low_high_cov_label = ['ORDINAL', 'CARDINAL', 'QUANTITY', 'NORP', 'WORK_OF_ART', 'EVENT']\n",
    "\n",
    "ft_q3 = df[df['label'].isin(low_high_cov_label)]\n",
    "ft_q3 = ft_q3[ft_q3['score'] < 0.7]\n",
    "ft_q3.sort_values('score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374aa5df",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Q4: Is confidence score correlated with text length or complexity?\n",
    "Find the correlation between length of texts and confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215ea9e",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get characters\n",
    "q4_chara_analysis = txt_len.copy()\n",
    "q4_chara_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871acc46",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "In this analysis, the tests that will be conducted are:\n",
    "1. Pearson correlation - to determine the linear relationship\n",
    "2. Spearman correlation - to determine the monotonic relationship\n",
    "\n",
    "**Null hypothesis:**\\\n",
    "There is no correlation between text length and confidence scores.\n",
    "\n",
    "**Alternative hypothesis:**\\\n",
    "There is a correlation between text length and confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d97089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson\n",
    "q4_pearson_corr, q4_p_value = stats.pearsonr(\n",
    "    q4_chara_analysis['chara'],\n",
    "    q4_chara_analysis['score']\n",
    ")\n",
    "\n",
    "# spearman\n",
    "q4_spearman_rho, q4_p_spearman = stats.spearmanr(\n",
    "    q4_chara_analysis['chara'],\n",
    "    q4_chara_analysis['score']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "print(f\"Spearman rho: {q4_spearman_rho:.3f}\")\n",
    "print(f\"Spearman p: {q4_p_spearman:.4f}\\n\")\n",
    "print(f\"Pearson r: {q4_pearson_corr:.3f}\")\n",
    "print(f\"Pearson p: {q4_p_value:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c73d523",
   "metadata": {},
   "source": [
    "Based on these readings,\n",
    "\n",
    "There is a highly significant (p < 0.05) weak positive monotonic and linear relationship (rho < 0.4, r < 0.4). This means that the effects of the text length to confidence score is negligible.\n",
    "\n",
    "Therefore, we reject the null hypothesis. However, the relationship has no practical importance.\n",
    "\n",
    "So, no fine-tuning can be uased by the text lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78f520",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Q5: Is confidence score correlated with text complexity?\n",
    "Find the correlation between the complexity of texts and confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107840ac",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ac983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ce50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "q5_complex_analysis = df[['text', 'score']].copy()\n",
    "q5_complex_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text processing\n",
    "text = q5_complex_analysis['text']\n",
    "words = text.str.split()\n",
    "word_count = words.str.len()\n",
    "word_count.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d340d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word diversity\n",
    "word_diversity = words.apply(set).str.len() / word_count\n",
    "word_diversity.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd89be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word length\n",
    "avg_word_length = words.str.join(' ').str.len() / word_count\n",
    "avg_word_length.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc06a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special character ratio\n",
    "special_char_ratio = text.str.count(r'[^\\w\\s]') / text.str.len()\n",
    "special_char_ratio.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average complexity\n",
    "q5_complex_analysis = q5_complex_analysis.assign(\n",
    "    complexity=(\n",
    "        word_diversity + \n",
    "        avg_word_length / 10 + \n",
    "        special_char_ratio\n",
    "    ) / 3  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc121fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "q5_complexity = q5_complex_analysis[['text', 'score', 'complexity']].sort_values('complexity')\n",
    "q5_complexity.sort_values('complexity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832eeef",
   "metadata": {},
   "source": [
    "#### Correlation Analysis\n",
    "In this analysis, the tests that will be conducted are:\n",
    "1. Pearson correlation - to determine the linear relationship\n",
    "2. Spearman correlation - to determine the monotonic relationship\n",
    "\n",
    "**Null hypothesis:**\\\n",
    "There is no correlation between text complexity and confidence scores.\n",
    "\n",
    "**Alternative hypothesis:**\\\n",
    "There is a correlation between text complexity and confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson\n",
    "q5_pearson_corr, q5_p_value = stats.pearsonr(\n",
    "    q5_complex_analysis['complexity'],\n",
    "    q5_complex_analysis['score']\n",
    ")\n",
    "\n",
    "# spearman\n",
    "q5_spearman_rho, q5_p_spearman = stats.spearmanr(\n",
    "    q5_complex_analysis['complexity'],\n",
    "    q5_complex_analysis['score']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n",
    "print(f\"Spearman rho: {q5_spearman_rho:.3f}\")\n",
    "print(f\"Spearman p: {q5_p_spearman:.4f}\\n\")\n",
    "print(f\"Pearson r: {q5_pearson_corr:.3f}\")\n",
    "print(f\"Pearson p: {q5_p_value:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a82e4a",
   "metadata": {},
   "source": [
    "From this: \n",
    "* There is a not significant (p > 0.05) positive monotonic relationship (rho < 0.4). - Spearman correlation test\n",
    "* There is a significant (p < 0.05) positive weak linear relationship (r < 0.4). - Pearson correlation test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
